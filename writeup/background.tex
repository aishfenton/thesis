%!TEX root = main.tex

% =====================
% CHAPTER
% ---------------------
% =====================
\chapter{Background}
\label{chap:background}

This chapter provides a short history and background material on the Vehicle Routing Problem. In particular we review the solution methods that have been brought to bear on the Vehicle Routing Problem and some of the classic results reported in the literature. 

This chapter is laid out as follows. We start in section \ref{sec:vo} by informally defining what the Vehicle Routing Problem is and by providing a timeline of the major milestones in its research. We also review a closely related problem, the Traveling Salesman Problem, which is a cornerstone of the Vehicle Routing Problem. We then review in section \ref{sec:em} the \emph{Exact Methods} that have been developed to solve the Vehicle Routing Problem. These are distinguished from the other methods we review in that they provide exact solutions, where the globally best answer is produced. We follow this in section \ref{sec:ch} by reviewing the classic \emph{Heuristics} methods that have been developed for the Vehicle Routing Problem. These methods aren't guaranteed to find the globally best answer, but rather aim to produce close to optimal solutions using algorithms with fast running times that are able to scale to large problem instances. In section \ref{sec:mh} we review \emph{Meta-heuristic} methods that have been adapted for the Vehicle Routing Problem. These methods provide some of the most competitive results available for solving the Vehicle Routing Problem and are considered state-of-the-art currently. Lastly, in section \ref{sec:si} we review a modern family of meta-heuristics called \emph{Swarm Intelligence} that has been inspired by the problem solving abilities exhibited by some groups of animals and natural processes. These last methods have become a popular area of research recently and are starting to produce competitive results to many problems. This thesis uses a Swarm Intelligence method for solving the Vehicle Routing Problem.


% =====================
% SECTION
% =====================
\section{Overview}
\label{sec:vo}

The Vehicle Routing Problem (commonly abbreviated to \VRP) describes the problem of assigning and ordering work for a finite number of resources, such that the cost of undertaking that work is minimised. Often the context used is that of a fleet of vehicles delivering goods to a set of customers, although the problem can equally be applied across many different industries and scenarios (including non-logistics scenarios, such as microchip layout). The aim is to split the deliveries between the vehicles and to specify an order in which each vehicle undertakes its work, such that the distance travelled by the vehicles is minimised and any pre-stated constraints are met. In the classic version of the \VRP\ the constraints that must be met are: 

\begin{enumerate}
   \item Each vehicle must start and end its route at the depot.
   \item All goods must be delivered.
   \item The goods can only be dropped off a single time and by a single vehicle.
   \item Each good requires a specified amount of capacity. However, each vehicle has a finite amount of capacity that cannot be exceeded. This adds to the complexity of the problem as it necessary influences the selection of deliveries assigned to each vehicle.
\end{enumerate}

\picscl{images/vrp.pdf}{An example of customers being assigned to three selected vehicle routes. The depot is the black dot in the centre.}{fig:vrp_example}{0.55}

More formally, the \VRP\ can be represented as a graph, $(V,E)$. The vertices of the graph, $V$, represent all locations that can be visited; this includes each customer location and the location of the depot. For convenience let $v^d$ denote the vertex that represents the depot. We denote the set of customers as $C = 1,2,...,n$. Next let the set of edges, $E$, correspond to the valid connections between customers and connections to the depot - typically for the \VRP\ all connections are possible. Each edge, $\edge{i,j} \in E$, has a corresponding cost $c_{ij}$. This cost is typically the travel distance between the two locations.

A solution to a given \VRP\ instance can be represented as a family of \emph{routes}, denoted by $\schd$. Each route itself is a sequence of customer visits that are performed by a single vehicle, denoted by $R = \seq{v_1, v_2,..., v_k}$ such that $v_i \in V$, and $v_1, v_k = v^d$. Each customer has a demand $d_i, i \in C$, and $q$ is the maximum demand permissible for any route (i.e. its maximum capacity). The cost of the solution, and the value we aim to minimise, is given by the following formula:

\[
   \sum_{R \in \schd} \sum_{v_i \in R} c_{v_i, v_{i+1}}
\]

We can now formalise the \VRP\ constraints as follows:

\begin{align}
   & \bigunion_{R \in \schd} = V \label{eq:vf1}\\
   & (R_i - v^d) \intersect R_j = \emptyset  && \forall R_i, R_j \in \schd \label{eq:vf2}\\
   & v_i = v_j                               && \forall R_i \in \schd, \lnot \exists v_i, v_j \in (R_i - v^d) \label{eq:vf3}\\
   & v_0, v_k \in R_i = v^d                  && \forall R_i \in \schd \label{eq:vf4}\\
   & \sum_{v \in R_i} d_v < q                && \forall R_i \in \schd \label{eq:vf5}
\end{align}

Equation \eqref{eq:vf1} specifies that all customers are included in at least one route. Equations \eqref{eq:vf2} and \eqref{eq:vf3} ensure that each customer is only visited once, across all routes. Equation \eqref{eq:vf4} ensures that each route starts and ends at the depot. Lastly, equation \eqref{eq:vf5} ensures that each route doesn't exceed its capacity. 

This version of the problem has come to be known as the Capacitated Vehicle Routing Problem (often appreciated to \CVRP\ in the literature). See chapter \ref{chap:pd} for an alternative formation, which states the problem as an integer linear programming problem, as is more standard in the \VRP\ literature\footnote{We believe that the formation provided in this chapter is simpler and more precise for understanding the algorithmic methods described in this chapter. However, we do provide a more standard formation in chapter \ref{chap:pd}}.

\VRP\ was first formally introduced in 1959 by Dantzig and Ramser in their paper, the \emph{Truck Scheduling Problem}\cite{Dantzig:1959}. The \VRP\ has remained an important problem in logistics and transport, and is one of the most studied of all combinatorial optimisation problems. Hundreds of papers have been written on it over the intervening 50 years. From the large number of implementations in use today it is clear that the \VRP\ has real benefits to offer transport and logistics companies. Anywhere from 5\% to 20\% savings have been reported where a vehicle routing procedure has been implemented \cite{TV2001}.

From the \VRP\ comes a family of related problems. These problems model other constraints that are encountered in real-world applications of the \VRP. Classic problems include: \VRP\ with Time Windows (\VRPTW), which introduces a time window constraint against each customer that the vehicle must arrive within; \VRP\ with Multiple Depots (\MDVRP), where the vehicles are dispatched from multiple starting points; and the Pickup and Delivery Problem (\PDP), where goods are both picked up and delivered during the course of the route (such as a courier would do).

% ====================
\subsection{\TSP\ Introduction and History}
\label{sec:tiah}

The \VRP\ is as a combination two problems, that are combinatorial hard in themselves: The Traveling Salesman Problem (more precisely the \MTSP), and the Bin Packing Problem, \BPP.

The Traveling Salesman Problem (\TSP) can informally be defined as follows: Given $n$ points on a map, provide a route through each of the $n$ points such that each point is only used once and the total distance travelled is minimised. The problem's name, the \emph{Traveling Salesman}, comes from the classic real world example of the problem. A salesman is sent on a trip to visit $n$ cities. They must select the order in which to visit the cities, such that they travel the least amount of distance.

Although the problem sounds like it might be easily solvable, it is in fact \nphard. The best known exact algorithms for solving the \TSP\ still require a running time of $\BIGO{2^n}$. Karp's famous paper, \emph{Reducibility Among Combinatorial Problems}\cite{Kar72}, in 1972 showed that the Hamiltonian Circuit problem is \npcomplete. This implied the NP-hardness of \TSP, and thus supplied the mathematical explanation for the apparent difficulty of finding optimal traveling salesman tours. 

\picscl{images/tsp.pdf}{Shown is an example of a 16 city \TSP\ tour around New Zealand's North Island. This tour is one of  20,922,789,888,000 possible tours for these 16 cities.}{fig:tsp}{0.65}

\TSP\ has a history reaching back many years. It is itself related to another classic graph theory problem, the Hamiltonian circuit. Hamiltonian circuits have been studied since 1856 by both Hamilton \cite{Hamilton:1856} and Kirkman \cite{Kirkman:1856}. Whereas the Traveling Salesman Problem has been informally discussed for many years\cite{Schrijver}, it didn't become actively studied until after 1928, where Menger, Whitney, Flood and Robinson produced much of the early results in the field. Robinson's RAND report\cite{Robinson:1949} is probably the first article to call the problem by the name it has since become known, the Traveling Salesman Problem. 

   \begin{quote}
   The purpose of this note is to give a method for solving a problem related to the traveling salesman problem. One formulation is to find the shortest route for a salesman starting from Washington, visiting all the state capitals and then returning to Washington. More generally, to find the shortest closed curve containing $n$ given points in the plane.
   \end{quote}

An early result was provided by Dantzig, Fulkerson, and  Johnson\cite{Dantzig:1954}. Their paper gave an exact method for solving a 49 city problem, a large number of cities for the time. Their algorithm used the cutting plane method to provide an exact solution. This approach has been the inspiration for many subsequent approaches, and is still the bedrock of algorithms that attempt to provide an exact solution.  

A generalisation of the \TSP\ is Multiple Traveling Salesman Problem (\MTSP), where multiple tours are constructed (i.e. multiple salesman can be used to visit the cities). The pure \MTSP\ can trivially be turned into a \TSP\ by constructing a graph $G$ with $n - 1$ additional copies of the starting vertex and by forbidding travel directly between the $n$ starting vertices. However, the pure formulation of \MTSP\ places no additional constraints on how the routes are constructed. Real life applications of the \MTSP\ typically require additional constraints, such as limiting the size or duration of each route (i.e. one salesman shouldn't be working a 12 hour shift, while another has no work).

\MTSP\ leads us naturally into the a family of problems given by the \VRP. \VRP, and it's family of related problems, can be understood as being a generalization of \MTSP\ that incorporates additional constraints. Some of these constraints, such as capacity limits, introduce additional dimensions to the problem that are in themselves hard combinatorial problems.

% =====================
% SECTION
% =====================
\section{Exact Methods}
\label{sec:em}

The first efforts at providing a solution to the \VRP\ were concerned with exact methods. These started by sharing many of the techniques brought to bear on \TSP. We follow Laporte and Nobert's survey\cite{LANO:87} and classify exact methods for the \VRP\ into three families: Direct tree search methods, Dynamic programming, and Integer linear programming.

The first classic Direct tree search results are due to Christolds and Ellison. Their 1969 paper provided the first branch and bound algorithm for exactly solving the \VRP\cite{CE:1969}. Unfortunately its time and memory requirements were large enough that it was only able to solve problems of up to 13 customers. This result was later improved upon by Christolds in 1976 by using a different branch model. This improvement allowed him to solve for up to 31 customers. 

Christofides, Mingozzi, and Toth\cite{CMT:1981}, provide a lower bound method that is sufficiently quick (in terms of runtime performance) to be used to as a lower bound for excluding nodes from the search tree. Using this lower bound they were able to provide solutions for problems containing up to 25 customers. Laporte, Mercure and Nobert\cite{LMN:1986} used \MTSP\ as a relaxation of the \VRP\ within a branch and bound framework to provide solutions for more realistically sized problems, containing up to 250 customers.  

The Dynamic Programming approach was first proposed for \VRP\ by Eilon, Watson-Gandy and Christofides (1971). Their approach allowed them to solve exactly for problems of 10 to 25 customers. Since then, Christofides has made improvements to this algorithm to solve exactly for problems up to 50 customers large.

A Set Partitioning method was given by Balinski, and Quandt in 1964\cite{balinski:64} to produce exact \VRP\ solutions. However, the problem sets they used were very small, only containing between 5 to 15 customers; And even then they weren't able to produce solutions for some of the problems. However, taking their approach as a starting point, many authors have been able to produce more powerful methods. Rao and Zionts \cite{RZ:1968}, Foster and Ryan\cite{FR:1976}, and Desrochers, Desrosiers and Solomon\cite{DMDJSM:1992} have all extended the basic set partitioning algorithm using the Column Generation method. These later papers have produced some of the better exact results. 

Notwithstanding the above, exact methods have been of more use in advancing the theoretical understanding of the \VRP\ than they have been to providing solutions to real life routing problems. This can mostly be attributed to the fact that real life \VRP\ instances often involve hundreds of customers, and involve richer constraints than are modelled in the classic \VRP.

% =====================
% SECTION
% =====================
\section{Classic Heuristics}
\label{sec:ch}

In this section we review the classic heuristic methods that have been developed for the \VRP. These methods are not guaranteed to find the globally best answer, but rather aim to produce close to optimal solutions using algorithms with fast running times that are able to scale to large problem instances. Classic heuristics for the \VRP\ can be classified into three families: Constructive heuristics; Two-phase heuristics, which can again be divided into two subfamilies, cluster first and then route, and route first and then cluster; and Improvement methods.

\subsection{Constructive Heuristics}
\label{subsec:conheu}

We start by looking at \emph{Constructive} heuristics. Constructive heuristics build a solution from the ground up. They typically provide a recipe for building each route, such that the total cost of all routes is minimised.

A trivial but intuitive constructive heuristic is the \emph{Nearest Neighbour} method. In this method routes are built-up sequentially. At each step the customer nearest to the last routed customer is chosen. This continues until the route reaches its maximum capacity, at which point a new route is started. In practice the Nearest Neighbour algorithm tends to provide poor results and is rarely used. 
 
\picscl{images/nn.pdf}{Shown is an example of the Nearest Neighbour method being applied. A partially constructed route selects customer $j$ to add, as its closest to the last added customer, $i$}{fig:nn}{0.55}

An early and influential result was given by Clarke and Wright in their 1964 paper\cite{clark:1964}. In their paper they present a heuristic extending Dantzig and Ramser's earlier work, which has since become known as the \emph{Clarke Wright Savings} heuristic. The heuristic is based on the simple premise of iteratively combining routes in order of those pairs that provide the largest saving. 

\picscl{images/cw.pdf}{Clark Wright Savings Algorithm. Customers $i,j$ are selected as candidates to merge. The merge results in a new route $R'$}{fig:cw_savings}{0.55}

The algorithm works as follows:

\begin{algorithm}[H]
   \caption{Clark Write Savings Algorithm}
   initialiseRoutes()\\
   $M$ = savingsMatrix($V$)\\
   $L$ = sortBySavings($SM$)\\
   \For{$l_{ij} \leftarrow L$}{
      $R^i, R^j$ = findRoutes($l_{ij}$)\\
      \If{feasibleMerge($R^i, R^j$)}{
         combineRoute($R^i, R^j$)
      }
   }
\end{algorithm}

The algorithm starts by initialising a candidate solution. For this it creates a route $R = \seq{v^d, v_i, v^d}$ for all $v \in V$. It then calculates a matrix $M$ that contains the savings $s_{ij} = c_{i0} + c_{j0} - c_{ij}$ for all edges $\edge{i,j} \in E$. It then produces a list, $L$, that enumerates each cell $i,j$ of the matrix in descending order of the savings. For each entry in the list, $l_{ij} \in L$, it selects the two routes, $R^i, R^j$, that contain customers $i,j \in V$ and tests to see if the two routes can be merged. A merge is permissible if and only if:

\begin{enumerate}
   \item $R^i \neq R^j$.
   \item $i,j$ are the first or last vertices (excluding the depot $v^d$) of their respective routes.
   \item The combined demand of the two routes doesn't exceed the maximum allowed $q$.
\end{enumerate}

The heuristic comes in two flavours, sequential and parallel. The sequential version adds the additional constraint that only one route can be constructed at a time. In this case one of the two routes considered, $R^i, R^j$, must be the route under construction. If neither of the routes are the route under construction then the list item is ignored and processing continues down the list. If the merge is permissible then we merge routes $R^i, R^j$ such that $R' = \seq{v_0,...,i,j,...,v_k}$. In the parallel version, once the entire list of savings has been enumerated then the resulting solution is returned as the answer. In the sequential version the \emph{for loop} is repeated until no feasible merges remain.

The Clark Write Savings heuristic has been used to solve problems of up to 1000 customers with results often within 10\% of optimal using only a 180 seconds of runtime\cite{TV2001}. The parallel version of the Clark Write Savings Algorithm outperforms the sequential version in most cases\cite{Laporte:1999} and is typically the one employed.

The heuristic has proven to be surprisingly adaptable and has been extended to deal with more specialised vehicle routing problems where additional objectives and constraints must be factored in. Its flexibility is a result of its algebraic treatment of the problem\cite{Laporte:1999}. Unlike many other \VRP\ heuristics that exploit the problem's spatial properties (such as many of the two-phase heuristics, see section \ref{subsec:conheu}), the savings formula can easily be adapted to take into consideration other objectives. An example of this is Solomon's equally ubiquitous algorithm \cite{Solomon:1987} which extends the Clark Wright Savings algorithm to cater for time constraints. 

This classic algorithm has been extended by Gaskell\cite{Gaskell:1967}, Yellow\cite{Yellow:1970} and Paessens\cite{Paessens:1988}, who have suggested alternatives to the savings formulas used by Clarke and Wright. These approaches typically introduce additional parameters to guide the algorithm towards selecting routes with geometric properties that are likely to produce better combinations. Alkinkemer and Gavish provide an interesting variation on the basic savings heuristic\cite{AG:1991}. They use a matching algorithm to combine multiple routes in each step. To do this they construct a graph such that each vertex represents a route, each edge represents a feasible saving, and the edges' weights represent the savings that can be realised by the merge of the two routes. The algorithm proceeds by solving a maximum cost weighted matching of the graph.

\subsection{Two-phase Heuristics}
\label{sec:tph}

We next look at two-phase heuristics. We start by looking at the cluster first, route second subfamily. One of the foundational algorithms for this method is due to Gillett and Miller who provided a new approach called the Sweep Algorithm in their 1974 paper\cite{GM:1974}. This popularised the two-phase approach, although a similar method was suggested earlier by Wren in his 1971 book, and subsequently in Wren and Holliday's 1972 paper for Operations Research Quarterly. In this approach, an initial clustering phase is used to cluster the customers into a base set of routes. From here the routes are treated as separate \TSP\ instances and optimised accordingly. The two-phase approach typically doesn't prescribe a method for how the \TSP\ is solved and assumes that already developed \TSP\ methods can be used. The classic sweep algorithm uses a simple geometric method to cluster the customers. Routes are built by sweeping a ray, centered at the depot, clockwise around the space enclosing the problem's locations. The Sweep method is surprising effective and has been shown to solve several benchmark \VRP\ problems to within 2\% to 9\% of the best known solutions\cite{TV2001}.

\picscl{images/sweep.pdf}{This diagram shows an example of the Sweep process being run. The ray is swept clockwise around the geographic area. In this example one route has already been formed, and a second is about to start at customer $i$.}{fig:sweep}{0.55}

Fisher and Jaikumars's 1981 paper\cite{FJ:1981} builds upon the two-phase approach by providing a more sophisticated clustering method. They solve a General Assignment Problem to form the clusters instead. A limitation of their method is that the amount of vehicle routes must be fixed up front. Their method often produces results that are 1\% to 2\% better than similar results produced by the classic sweep algorithm\cite{TV2001}. 

Christofides, Mingozzi, and Toth expanded upon this approach in \cite{CMT:1981} and proposed a method that uses a truncated branch and bound technique (similar to Christofides's Exact method). At each step it builds a collection of candidate routes for a particular customer, $i$. It then evaluates each route by solving it as a \TSP, from which it then selects the shortest \TSP\ as the route.

The Petal algorithm is a natural extension to the Sweep Algorithm. It was first proposed by Balinski and Quandt \cite{balinski:64} and then extended by Foster and Ryan \cite{FR:1976}. The basic process is to produce a collection of overlapping candidate routes (called petals) and then to solve a set partitioning problem to produce a feasible solution. As with other two-phase approaches it's assumed that the order of the customers within each route is solved using an existing \TSP\ heuristics. The petal method has produced competitive results for small solutions, but quickly becomes impractical where the set of candidate routes that must be considered is large.  

Lastly, there are route first, cluster second methods. The basic premise of these techniques are to first construct a `grand' \TSP\ tour such that all customers are visited. The second phase is then concerned with splitting this tour into feasible routes. Route first, cluster second methods are generally thought to be less competitive than other methods\cite{Laporte:1999}, although interestingly, Haimovich and Rinnooy Kan have shown that if all customers have unit demand then a simple shortest path algorithm (which can be solved in polynomial time) can be used to produce a solution from a \TSP\ tour that is asymptotically optimal\cite{HK:1985}.

\subsection{Iterative Improvement Heuristics}

Iterative improvement methods follow an approach where an initial candidate solution is iteratively improved by applying an operation that improves the candidate solution, typically in a small way, many thousands of times. The operations employed are typically simple and only changes a small part of the candidate solution, such as the position of a single customer or edge within the solution. The set of solutions that is obtainable from a current candidate solution, $S$, by applying an operator $Op$ is known as $S$'s neighbourhood. Typically with \emph{Iterative Improvement} heuristics, a new solution $S'$ is selected by exhaustively searching the entire neighbourhood of $S$ for the best improvement possible. If no improvement can be found then the heuristic terminates. The initial candidate solution (i.e. the starting point of the algorithm) can be randomly selected or can be produced using another heuristic. The \emph{Constructive Heuristics} are typically used for initially seeding an improvement heuristic, see \ref{sec:ch} for more information on these.
 
Probably one of the best known improvement operators is 2-Opt. The 2-Opt operator takes two edges $\edge{i,j}, \edge{k,l} \in T$, where $T$ are the edges traversed by a particular route $R = \seq{v_1, ..., v_i, v_j, ..., v_k, v_l, ..., v_n}$, and removes these from the candidate solution. This splits the route into two disconnected components, $D_1 = \seq{v_j, ..., v_k}, D_2 = \seq{v_1, ..., v_i,v_l, ..., v_n}$. A new candidate solution is produced by reconnecting $D_1$ to $D_2$ using the same vertices $i,j,l,k$ but with alternate edges, such that $\edge{i,j}, \edge{j,l} \in T$.

\picscl{images/2opt.pdf}{This diagram shows 2-Opt being applied to a candidate solution $R$ and producing a new solution $R'$. In this example edges $\edge{i,j}, \edge{k,l}$ are exchanged with edges $\edge{i,k}, \edge{j,l}$.}{fig:2opt}{0.55}

The rationale behind 2-Opt is that, due to the triangle inequality, edges that cross themselves are unlikely to be optimal. 2-Opt aims to detangle a route.

There are a number of other operations suggested in the literature. Christofides and Eilon give one of the earliest iterative improvement methods in their paper\cite{CE:1969}. In the paper they make a simple change to 2-Opt to increase the amount of edges removed from two to three -- the operation fittingly being called 3-Opt. They found that their heuristic produced superior results than 2-Opt.

In general, operations such as 3-Opt, that remove edges and then search for a more optimal recombination of components take \BIGO{n^y} where $y$ is the number of edges removed. A profitable strain of research has focused on producing operations that reduce the amount of recombinations that must be searched. Or presents an operation that has since come to be known as Or-Opt\cite{Or:1976}. Or-Opt is a restricted 3-Opt. It searches for a relocation of all sets of 3 consecutive vertices (which he calls chains), such that an improvement is made. If an improvement can't be made then it tries again with chains of 2 consecutive vertices, and so on. Or-Opt has been shown to produce similar results to that of 3-Opt, but with a running time of \BIGO{n^2}. More recently Renaud, Boctor, and Laptorte have presented a restricted version of 4-Opt (in a similar vein to Or-Opt) that has a running time of \BIGO{wn^2}\cite{RBL:1996}. 

Iterative improvement heuristics are often used in combination with other heuristics. In this case they are run on the candidate solution after the initial heuristic has completed. However, if used in this way there is often a fine balance between producing an operation that improves a solution, and one that is sufficiently destructive enough to escape a local minimum. Interest in Iterative Improvement heuristics has grown as the operations developed for them, such as $Or-Opt$, are directly applicable to more modern heuristics, such as the family known as meta-heuristics presented in the next section. 

% =====================
% SECTION
% =====================
\section{Meta-heuristics}
\label{sec:mh}

Meta-Heuristics are a broad collection of methods that make few or no assumptions about the type of problem being solved. They provide a framework that allows for individual problems to be modelled and `plugged in' to the meta-heuristic. Typically meta-heuristics take an approach where a candidate solution (or solutions) is initially produced and then is iteratively refined towards the optimal solution. Intuitively meta-heuristics can be thought of searching a problem's search space. Each iteration searches the neighbourhood of the current candidate solution(s) looking for new candidate solutions that move closer to the global optimum.

\picscl{images/good_search_space.pdf}{This diagram shows an example of a search space that a meta-heuristic moves through. In this example the peak at the centre of the figure is the globally best answer, but there are also hills and valleys that the meta-heuristic may become caught in. These are called local minima and maxima.}{fig:gss}{0.66}

A limitation of meta-heuristics is that they aren't guaranteed to find an optimal solution (or even a good one!). Moreover, the theoretical underpinnings of what makes one meta-heuristic more effective than another are still poorly understood. Meta-heuristics within the literature tend to be tuned for specific problems and then validated empirically.

There have been a number of meta-heuristics produced for the \VRP\ in recent years and many of the most competitive results produced in the last ten years have been due to them. We next review some of the more well known meta-heuristic results for the \VRP.

\subsection{Simulated Annealing}

Simulated Annealing is inspired by the annealing process used in metallurgy. The algorithm starts with a candidate solution (which can be randomly selected) and then moves to nearby solutions with a probability dependent on the quality of the solution and a global parameter $T$, which is reduced over course of the algorithm. In classic implementations the following formula is used to control the probability of a move: 

\[
e^{-\frac{f(s')-f(s)}{T}}
\]

Where $f(s)$ and $f(s')$ represent the solution quality of the current solution, and the new solution respectively. By analogy to the metallurgy process, $T$ represents the current temperature of the solution. Initially $T$ is set to a high value. This lets the algorithm free itself from any local optima that it may be caught in. It is then cooled over the course of the algorithm forcing the search to converge on a solution. 

One of the first results for the \VRP\ was given by Robuste, Daganzo and Souleyrette\cite{RDS:1990}. They define the search neighbourhood as being all solutions that can be obtained from the current solution by applying one of two operations: relocating part of a route to another position within the same route, or exchanging customers between routes. They tested their solution on some large real-world instances of up to 500 customers. They reported some success with their approach, but as their test cases were unique, no direct comparison is possible. 

Osman has given the best known Simulated Annealing results for the \VRP\cite{Osman:1993}. His algorithm expands upon many areas of the basic Simulated Annealing approach. The method starts by using the Clark and Wright algorithm to produce an initial position. It defines its neighbourhood as being all candidate solutions that can be reached by applying an operator he names the $\lambda$-interchange operation.

$\lambda$-interchange works by selecting two sequences (i.e. chains) of customers $C_p, C_q$ from two routes, $R_p$ and $R_q$, such that $\length{C_p}, \length{C_q} < \lambda$ (note that the chains aren't necessarily of the same length). The customers within each chain are then exchanged with each other in turn, until an exchange produces an infeasible solution. As the neighbourhood produced by $\lambda$-interchange is typically quite large, Osman restricts $\lambda$ to being less than 2 and suggests that the first move that provides an improvement is used rather than exhaustively searching the entire neighbourhood.

\picscl{images/lambdaopt.pdf}{This diagram shows an example of $\lambda$-interchange being applied to a candidate solution. Two sequences of customers $C_p$ and $C_q$ are selected from routes $R_p$, $R_q$ respectively. Customers from $C_p$ are then swapped with $C_q$ where feasible.}{fig:lambdaopt}{0.66}

Osman also uses a sophisticated cooling schedule. His main change being that the temperature is cooled only while improvements are found. If no improvement is found then he resets the temperature using $T_i = max(\frac{T_r}{2}, T_b)$, where $T_r$ is the reset temperature, and $T_b$ is temperature of the best solution found so far. 

Although Simulated Annealing has produced some good results, and in many cases outperforms classic heuristics (compare \cite{Laporte:1999} with \cite{GLP:1999}), it is not competitive with the Tabu Search methods discussed in section \ref{sec:ts}.

\subsection{Genetic Algorithms}

Genetic Algorithms were first proposed in \cite{Holland:1975}. They have since been applied to many problem domains and are particularly well suited to applications that must work across a number of distinct domains. In fact they were the first evolutionary-inspired algorithm to be applied to combinatorial problems\cite{Potvin:2009}. The basic operation of a GA is as follows:

\begin{algorithm}[H]
   \caption{Simple Genetic Algorithm}
   Generate the initial population\\
   \While{termination condition not met}{
      Evaluate the fitness of each individual\\
      Select the fittest pairs\\
      Mate pairs and produce next generation\\
      Mutate (optional)\\
   }
\end{algorithm}

In a classic Genetic Algorithm each candidate solution is encoded as a binary string (i.e. chromosome). Each individual (i.e. candidate solution) is initially created randomly and used to seed the population. A technique often employed in the literature is to initially `bootstrap' the population by making use of another heuristic to produce the initial population. However, special care must be taken with this approach to ensure that diversity is maintained across the population, as you risk premature convergence by not introducing enough diversity in the initial population. 

Next, the fittest individuals are selected from the population and are mated the next generation. The mating process uses a special operator called a \emph{crossover} operator that takes two parents and produces offspring from these by combining parts of each parent. Optionally, a mutation operation is also applied, that introduces a change that doesn't exist in either parent. The classic crossover operation takes two individuals encoded as binary strings and splits these at one or two points along the length of the string. The strings are then recombined to form a new binary string, which in turn encodes a new candidate solution. The entire process is continued until a termination condition is met (often a predetermined running time), or until the population has converged on a single solution.

Special consideration needs to be given to how problems are encoded and to how the crossover and mutation operators work when using Genetic Algorithms to solve discrete optimisation problems, such as the \VRP. For example, the classic crossover operation, which works on binary strings, would not work well on a \TSP\ path. When two parts of two solutions are combined in this way they are likely to contain duplicates. Therefore, it is more common for the \VRP\ (and the \TSP) to use a direct representation and to use specially designed crossover operators. In this instance the \VRP\ is represented as a set of sequences, each holding an ordered list of customers. The crossover operators are then designed so that they take into consideration the constraints of the \VRP.

Two commonly used crossover operators are the Order Crossover ($OX$) and the Edge Assembly Crossover ($EAX$). $OX$\cite{OSH:1987} operates by selecting two cut points within each route. The substring between the two cut points is copied from the second parent directly into the offspring. Likewise, the string outside the cut points is copied from the first parent into the offspring, but with any duplicates removed. This potentially leaves a partial solution, where not all customers have been routed. The partial solutions is then repaired by inserting any unrouted customers into the child in the same order that they appeared in the second parent.

\picscl{images/ox.pdf}{This diagram shows the $OX$ crossover operator being applied to two tours from a \TSP. A child is produced by taking customers at position $3, 4, 5$ from the second parent and injecting these into the same position in the first parent, removing any duplicates. This leaves customers $4,5$ unrouted, so they are reinserted back into the child in the order that they appear in the first parent.}{fig:ox}{0.75}

Another common crossover operator is $EAX$. $EAX$ was originally designed for the \TSP\ but has been adapted to the \VRP\ by \cite{Nagata:2007}. $EAX$ operates using the following process:

\begin{enumerate}
    \item Combine the two candidate solutions into a single graph by merging each solution's edge sets.
    \item Create a partition set of the graph's cycles by alternately selecting an edge from each graph.
    \item Randomly select a subset of the cycles.
    \item Generate a (incomplete) child by taking one of the parents and removing all edges from the selected subset of cycles, then add back in the edges from the parent that wasn't chosen. 
    \item Not all cycles in the child are connected to the route. Repair them by iteratively merging the disconnected cycles to the connected cycles.
\end{enumerate}

\picscl{images/eax.pdf}{This diagram shows an example of of $EAX$ being applied on two parent solutions, $P1$ and $P2$. The parents are first merged together. Then a new graph is created by selecting alternate edges from each parent $P1$, $P2$. A subset of cycles are then taken and applied to $P1$, such that any edges from $P1$ are removed. The child solution produced is infeasible (it contains broken routes). These would need to be repaired.}{fig:eax}{0.40}

An alternative and interesting approach found in the literature is to instead encode a set of operations and parameters that are fed to another heuristic, that in turn produces a candidate solution. A well known example of this approach was suggested in \cite{BW:1993} who encoded an ordering of the customers. The ordering is then fed into an Insertion heuristic to produce the actual candidate solutions.

An influential result that uses Genetic Algorithms to solve \VRPTW\ is given in \cite{TNJ:1991} with their GIDEON algorithm. GIDEON uses an approach inspired by the sweep heuristic (an overview is the sweep method is provided in section \ref{sec:tph}). It builds routes by sweeping a ray, centered at the depot, clockwise around the space enclosing the customer's locations. Customers are collected into candidate routes based on a set of parameters that are refined by the Genetic Algorithm. GIDEON uses the Genetic Algorithm to evolve the parameters used by the algorithm, rather than to operate on the problem directly. Finally GIDEON uses a local search method to optimise customers within each route, making use of the $\lambda$-interchange operator (A description of this operator is provided in section \ref{sec:ts}).

Generally speaking, Genetic Algorithms aren't as competitive as other meta-heuristics at solving the \VRP. However, more recently there have been two very promising applications of Genetic Algorithms being used to solve the \VRP. Nagata \cite{Nagata:2007} has adapted the EAX operator for use with the \VRP. And Berger and Barkaoui have presented a Hybrid Genetic Algorithm called HGA-VRP in \cite{BM:2003}. HGA-VRP adapts a construction heuristic to use it as a crossover operator. Their basic idea is that a set of routes are selected from each parent that are located close to each other. Customers are then removed from one parent and inserted into the second using an operation inspired by Solomon's construction heuristic for \VRPTW \cite{Solomon:1987}.

Both methods have reached the best known solution for a number of the classic \VRP\ benchmark instances by Christofides, Mingozzi and Toth \cite{CMT:1981} and are competitive with the best Tabu Search methods.

\subsection{Tabu Search}
\label{sec:ts}

Tabu Search follows the general approach shared by many meta-heuristics; it iteratively improves a candidate solution by searching for improvements within the current solution's neighborhood. Tabu search starts with a candidate solution, which may be generated randomly or by using another heuristic. Unlike Simulated Annealing, the best improvement within the current neighbourhood is always taken as the next move. This introduces the problem of cycling between candidate solutions. To overcome this Tabu Search introduces a list of solutions that have already been investigated and are forbidden as next moves (hence its name).

The first instance of Tabu Search being used for \VRP\ is by Willard\cite{Willard:1989}. Willard's approach made use of the fact that \VRP\ instances can be transformed into \MTSP\ instances and solved. The algorithm makes use of a combination of simple vertex exchange and relocation operations. Although opening the door for further research, its results weren't competitive with the best classic heuristics. 

Osman gives a more competitive use of Tabu Search in \cite{Osman:1993}. As with his Simulated Annealing method he makes use of the $\lambda$-interchange operation to define the search neighbourhood. Osman provides two alternative methods to control how much of the neighbourhood is searched for selecting the next move: Best-Improvement (BI) and First-Improvement (FI). Best-Improvement searches the entire neighbourhood and selects the move that is the most optimal. First-Improvement searches only until a move is found that is more optimal than the current position. Osman's heuristic produced competitive results that out performed many other heuristics. However, it has since been refined and improved upon by newer Tabu Search methods.

Toth and Vigo introduced the concept of Granular Tabu Search (GTS)\cite{GHL:1998}. Their method makes use of a process that removes moves from the neighbourhood that are unlikely to produce good results. They reintroduce these moves back into the process if the algorithm is stuck in a local minimum. Their idea follows from an existing idea known as Candidate Lists. Toth and Vigo's method has produced many competitive results.

Taillard has provided one of the most successful methods for solving the \VRP\ in his Tabu Search method in \cite{Taillard:1993}. Talliard's Tabu Search uses Or's $\lambda$-interchange as its neighbourhood structure. It borrows two novel concepts from \cite{GHL:1994}: The use of a more sophisticated tabu mechanism, where the duration (or number of iterations) that an item is tabu for is chosen randomly; And a diversification strategy, where vertices that are frequently moved without giving an improvement are penalised. A novel aspect of Taillard's algorithm is its decomposing of the problem into sub-problems. Each problem is split into regions using a simple segmentation of the region centred about the depot (Taillard also provides an alternative approach for those problems where the customers are not evenly distributed around the depot). From here each subproblem is solved individually, with customers being exchanged between neighboring segments periodically. Taillard observes that exchanging customers beyond geographically neighboring segments is unlikely to produce an improvement, so these moves are safely ignored. Taillard's method has produced some of the currently best known results for the standard Christofides, Mingozzi and Toth problem sets \cite{CMT:1981}. 

\subsection{Large Neighbourhood Search}

Large Neighbourhood Search (commonly abbreviated to LNS) was recently proposed as a heuristic by Shaw\cite{Shaw:1998}. Large Neighbourhood Search is a type of heuristic belonging to the family of heuristics known as Very Large Scale Neighbourhood search (VLSN)\footnote{LNS is somewhat confusingly named given that it a type of VLSN, and not a competing approach.}. Very Large Scale Neighborhood search is based on a simple premise; rather than searching within a neighborhood of solutions that can be obtained from a single (and typically quite granular) operation, such as 2-opt, it might be profitable to consider a much broader neighborhood - a neighbourhood of candidate solutions that are obtained from applying many simultaneous changes to a candidate solution. What distinguishes these heuristics from others is that the neighborhoods under consideration are typically exponentially large, often rendering them infeasible to search. Therefore much attention is given to providing methods that can successfully traverse these neighborhoods. 

Large Neighbourhood Search uses a Destroy and Repair metaphor for how it searches within its neighbourhood. Its basic operation is as follows:

\begin{algorithm}[H]
   \caption{Large Neighbourhood Search}
   $x$ = an initial solution\\
   \While{termination condition not met}{
      $x^t$ = $x$\\
      $destroy(x^t)$ \\
      $repair(x^t)$ \\
      \If {$x_t$ better than current solution}{
         $x = x_t$
      }
   }
   \KwResult{ $x$ }
\end{algorithm}

It starts by selecting a starting position. This can be done randomly or by using another heuristic. Then for each iteration of the algorithm a new position is generated by destroying part of the candidate solution and then by repairing it. If the new solution is better than the current solution, then this is selected as the new position. This continues until the termination conditions are met. Large Neighbourhood Search can be seen as being a type of Very Large Scale Neighborhood search because at each iteration the number of neighboring solutions is exponential large, based on the number of items removed (i.e. destroyed).

Obviously the key components of this approach are the functions used to destroy and repair the solution. Care must be given to how these functions are constructed. They must pinpoint an improving solution from a very large neighbourhood of candidates, while also providing enough degrees of freedom to escape a local optimum.

Empirical evidence in the literature shows that even surprisingly simple destroy and repair functions can be effective -- more effective in some cases\cite{Shaw:1998}\cite{Ropke:2005}. In applications of Large Neighbourhood Search for \VRP\ a pair of simple operations are commonly used (often alongside more complex ones too) for the destroy and repair functions. Specifically, the solution is destroyed by randomly selecting and removing $n$ customers. It is then repaired by finding the least cost reinsertion points back into the solution of the $n$ customers.

Shaw applied Large Neighbourhood Search to \VRP\ in his original paper introducing the method\cite{Shaw:1998}. Shaw introduced a novel approach for his destroy and repair functions. The destroy function removes a set of `related' customers. He defines a related customer to be any two customers that share a similar geographic location, that are sequentially routed, or that share a number of similar constraints (such as overlapping time windows if times constraints are used). The idea of removing related customers, over simply removing random customers, is that related customers are more likely to be profitably exchanged (moreover, unrelated customers are more likely to be reinserted back in their original positions). Shaw's repair function made use of a simply branch and bound method that finds the minimum cost reinsertion points within the partial solution. His results where immediately impressive and reached many of the best known solutions on the Christofides, Mingozzi and Toth problems\cite{CMT:1981}.

More recently Ropke proposed an extension to the basic Large Neighbourhood Search process in \cite{Ropke:2005}. His method adds the concept of using a collection of destroy and repairs functions, rather than using a single pair. Which function to use is selected at each iteration based on its previous performance. In this way the algorithm self adapts to using the most effective function to search the neighbourhood. 

Ropke makes use of several destroy functions. He uses a simple random removal heuristic, Shaw's removal heuristic, and a \emph{worst} removal heuristic, which removes the most costly customers (in terms of that customer's contribution the route's overall cost). Likewise, he makes use of several different insertion functions. These include a simple greedy insertion heuristic, and a novel insertion method he calls the `regret heuristic'. Informally, the regret heuristic reinserts those customers first who are most impacted (in terms of increased cost) by not being inserted into their optimum positions. Specifically, let $U$ be the set of customers to be reinserted and let $x_{ik} = \set{1,...,m}$ be a variable that gives the $k$'th lowest cost for inserting customer $i \in U$ into the partial solution. Now let $c_i^* = x_{i2} - x_{i1}$, in other words the cost difference between inserting customer $i$ into its second best position and its first. Now in each iteration of the repair function choose a customer that maximises:
\[
   \operatorname*{max}_{i \in U} c_i^*
\]

Ropke presents a series of results that show that his Large Neighbourhood Search is very competitive for solving the \VRP\ and its related problems (i.e. \VRPTW, \PDPTW, and \DARP). Considering that Large Neighbourhood Search was only proposed in 1998, it has been very successful. In a short space of time it has attracted a large amount of research and has produced some of the most competitive results for solving the \VRP.

% =====================
% SECTION
% =====================
\section{Swarm intelligence}
\label{sec:si}

A recent area of research is in producing heuristics that mimic certain aspects of swarm behaviour. Probably the most well known heuristics in this family are Particle Swarm Optimisation (PSO) and Ant Colony Optimisation (ACO). Real life swarm intelligence is interesting to combinatorial optimisation researchers as it demonstrates a form of emergent intelligence, where individual members with limited reasoning capability and simple behaviours, are still arrive at optimal solutions to complex resource allocation problems.

In the context of combinatorial optimisation, these behaviours can be mimicked and exploited. Algorithms that make use of this approach produce their solutions by simulating behaviour across a number of agents, who in themselves, typically only perform rudimentary operations. A feature of this class of algorithms is the ease with which they can be parallelised, making them more easily adaptable to large scale problems.

Swarm Intelligence algorithms have been employed to solve a number of problems. We look at two examples here, Ant Colony Optimisation and the Bees Algorithm, which this thesis makes use of.

\subsection{Ant Colony Optimisation}

Ant Colony Optimization is inspired by how ants forage for food and communicate promising sites back to their colony. Real life ants initially forage for food randomly. Once they find a food source they return to the colony and in the process lay down a pheromone trail. Other ants that then stumble upon the pheromone trail follow it with a probability dependent on how strong (and therefore how old) the pheromone trail is. If they do follow it and find food, they then return too thus strengthening the pheromone trail. The strength of the pheromone trail reduces over time meaning that younger and shorter pheromone trails, that do not take as long to traverse, attract more ants.  

\picscl{images/aco.pdf}{This diagram depicts how ants make use of pheromone trails to optimise their exploitation of local food sources.}{fig:aco}{1}

Ant Colony Optimisation mimics this behaviour on a graph by simulating ants marching along a graph that represents the problem being solved. The basic operation of the algorithm is as follows: 

\begin{algorithm}[H]
   \caption{Ant Colony Optimisation}
   \KwData{A graph representing the problem}
   \While{termination condition not met}{
      positionAnts()\\
      \While{solution being built}{
         marchAnts()\\
      }
      updatePheromones()\\
   }
\end{algorithm}
 
At each iteration of the algorithm the ants are positioned randomly within the graph. The ants are then stochastically marched through the graph until they have completed a candidate solution (in the case of a \TSP\ this would be a tour of all vertices). At each stage of the march each ant selects their next edge based on the following probability formula:

\[
   p_{ij}^k = \frac{ [\tau_{ij}^{\alpha}] [\eta_{ij}^{\beta}] }{ \sum_{l \in N^k} [\tau_{il}^{\alpha}] [\eta_{il}^{\beta}] }
\]

Where $p_{ij}^k$ is the probability that ant $k$ will traverse edge \edge{i,j}, $N^k$ is the set of all edges that haven't been traversed by ant $k$ yet, $\tau$ is the amount of pheromone that has been deposited at an edge, $\eta$ is the desirability of an edge (based on a priori knowledge specific to the problem), and $\alpha$ and $\beta$ are global parameters that control how much influence each term has.

Once the march is complete and a set of candidate solutions have been constructed (by each ant, $k$), update the pheromones deposited on each edge using the following equation:

\[
   \tau_{ij} = (1 - \rho) \tau_{ij} + \sum_{k=1}^m \Delta \tau_{ij}^k
\]

Where $0 < \rho \le 1$ is the pheromone persistence, and $\Delta \tau_{ij}^k$ is a function that gives the amount of pheromone deposited by ant $k$. The function is defined as:

\[
   \Delta \tau_{ij}^k = \left\{
   \begin{array}{l l}
     1 / C^k & \quad \text{if edge $\edge{i,j}$ is visited by ant $k$} \\
     0       & \quad \text{otherwise} \\
   \end{array} \right.
\]

Where $C^k$ represents the total distance traveled through the graph by ant $k$. This ensures that shorter paths result in more pheromone being deposited.

As an example of Ant Colony Optimisation's use in combinatorial problems, we show how it can be applied to the \TSP. We build a weighted graph with $i \in V$ representing each city to be visited and $\edge{i,j} \in E$ and $w_{ij}$ representing the cost of travel between each city. Then at each step of the iteration we ensure that the following constraints are met:

\begin{itemize}
   \item Each city is visited at most once.
   \item We set $\eta_{ij}$ to be equal to $w_{ij}$.
\end{itemize}

When the Ant Colony Optimiser starts, it positions each ant at a randomly selected vertex (i.e. city) within the graph. Each step of an ant's march then builds a tour through the cities. Once an ant has completed a tour we have then produced a candidate solution for the \TSP. Initially the solutions will be of low quality, so we use the length of the tours to ensure that more pheromone is deposited on the shorter tours. At the end of $n$ iterations the ants will have converged on a near optimal solution (but like all meta-heuristics there's no guarantee that this will be the global optimum).

\picscl{images/acotsp.pdf}{Shown is an example of ACO being used to solve a \TSP. Initially the ants explore the entire graph. At the end of each iteration the more optimal tours will have more pheromone deposited on them, meaning in the next iteration the ants are more likely to pick these edges when constructing their tour. Eventually the ants converge on a solution.}{fig:acotsp}{0.66}
 
Ant Colony Optimisation has been applied to \VRP\ by Bullnheimer, Hartl, and Strauss in \cite{BHS:1999A} \cite{BHS:1999B}. They adapted the straight forward implementation used for the \TSP,detailed immediately above, by forcing the ant to create a new route each time it exceeds the capacity or maximum distance constraint. They also use a modified edge selection rule that takes into account the vehicle's capacity and its proximity to the depot. Their updated rule is given by:

\[
   p_{ij}^k = \frac{ [\tau_{ij}^{\alpha}] [\eta_{ij}^{\beta}] [s_{ij}] [\kappa_{ij}]  }{ \sum_{l \in N^k} [\tau_{il}^{\alpha}] [\eta_{il}^{\beta}] [s_{il}] [\kappa_{il}] }
\]

Where $s$ represents the proximity of customers $i, j$ to the depot, and $\kappa = (Q^i + q^j) / Q$ - $Q$, giving the max capacity, $Q^i$ giving the capacity already used on the vehicle, and $q^j$ is the additional load to be added. $\kappa$ influences the ants to take advantage of the available capacity. 

Bullnheimer et al.'s implementation of Ant Colony Optimisation for \VRP\ produces good quality solutions for the Christofides, Mingozzi and Toth problems \cite{CMT:1981}, but is not competitive with the best modern meta-heuristics.

More recently Reimann, Stummer, and Doerner have presented a more competitive implementation of Ant Colony Optimisation for \VRP\ \cite{RSD:2002}. Their implementation operates on a graph where $\edge{i,j} \in E$ represent the savings of combining two routes, as given by the classic Clark and Wright Savings heuristic (see section \ref{subsec:conheu} for more on this heuristic). Each ant selects an ordering of how the merges are applied. This implementation is reported to be competitive with the best meta-heuristics \cite{Potvin:2009}.   

\subsection{Bees Algorithm}
\label{subsec:beesalgorithm}

Over the last decade, and inspired by the success of Ant Colony Optimisation, there have been a number of algorithms proposed that aim exploit the collective behaviour of bees. This includes: \emph{Bee Colony Optimisation}, which has been applied to many combinatorial problems, \emph{Marriage in Honey Bees Optimization} (MBO) that has been used to solve propositional satisfiability problems, \emph{BeeHive} that has been used for timetabling problems, the \emph{Virtual Bee Algorithm} (VBA) that has been used for function optimisation problems, \emph{Honey-bee Mating Optimisation} (HBMO) that has been used for cluster analysis, and finally, the \emph{Bees Algorithm} that is the focus of this thesis. See \cite{LJDS:2009} for a bibliography and high level overview on many of these algorithms. 

The Bees Algorithm was first proposed in \cite{PGKORZ:2005}. It is inspired by the foraging behaviour of honey bees. Bee colonies must search a large geographic area around their hive in order to find sites with enough pollen to sustain a hive. It is essential that the colony makes the right choices in which sites are exploited and how much resource is expended on a particular site. They achieve this by sending scout bees out in all directions from the hive. Once a scout bee has found a promising site it then returns to the hive and recruits hive mates to forage at the site too. The bee does this by performing a \emph{waggle} dance. The dance communicates the location and quality of the site (i.e. fitness). Over time, as more bees successfully forage at the site, more are recruited to exploit the site - in this aspect bee behaviour shares some similarities with ants foraging behaviour. 

\picscl{images/waggle.pdf}{Shown is the \emph{waggle} dance performed by a honey bee (image courtesy of \cite{biodidac:website}). The direction moved by the bee indicates the angle that the other bees must fly relative to the sun to find the food source. And the duration of the dance indicates its distance.}{fig:waggle}{0.12}

Informally the algorithm can be described as follows. Bees are initially sent out to random locations. The fitness of each site is then calculated. A proportion of the bees are reassigned to those sites that had the highest fitness values. Here each bee searches the local neighbor of the site looking to improve the site's fitness. The remainder of the bees are sent out scouting for new sites, or in other words, they are set to a new random position. This process repeats until one of the sites reaches a satisfactory level of fitness -- or a pre determined termination condition is met.

More formally, the algorithm operates as follows:

\begin{algorithm}[H]
   \caption{Bees Algorithm}
   $B = \set{b_1, b_2,..., b_n}$\\
   setToRandomPosition($B$)\\
   \While{termination condition not met}{
      sortByFitness($B$) \\
      $E = \set{b_1, b_2,...,b_e}$ \\
      $R = \set{b_{e+1}, b_{e+2},...,b_{m}}$ \\
      searchNeighbourhood($E \union \set{c_1,...,c_{nep}}$) \\
      searchNeighbourhood($R \union \set{d_1,...,d_{nsp}}$) \\
      setToRandomPosition($B - (E \union R)$) \\
   }
\end{algorithm}

$B$ is the set of bees that are used to explore the search space. Initially the bees are set to random positions. Function $sortByFitness$ sorts the bees in order of maximum fitness. It then proceeds by taking the $m$ most promising sites found by the bees. It does this by partitioning these into two sets, $E, N \subset B$. $E$ is the first $e$ best sites, and represents the so called \emph{elite} bees. $N$ is the $m - e$ next most promising sites. The $searchNeighbourhood$ function explores the neighbourhood around a provided set of bees. Each site in $E$ and $N$ is explored. $nep$ bees are recruited for the search of each $b \in E$, and $nsp$ are recruited for the search of each $b \in N$. In practice this means that $nep$ and $nsp$ moves are explored within the neighbourhoods of $E$ and $N$'s sites, respectively. These moves are typically made stochastically, but it is possible for a deterministic approach to be used too. The remaining $n - m$ bees (in other words, those not in $E$ and $N$) are set to random positions. This is repeated until the termination condition is met, which may be a running time threshold or a predetermined fitness level. 

The advantage promised by the Bees Algorithm over other meta-heuristics is its ability to escape local optima and its ability to navigate search topologies with rough terrain (such as in figure \ref{fig:rtst}). It achieves this by scouting the search space for the most promising sites, and then by committing more resource to the exploration of those sites that produce better results.

\picscl{images/rough_search_space.pdf}{Shown is a search space with many valleys and hills. These search spaces provide a challenge to meta-heuristic approaches as there are many local minima and maxima to get caught in. The Bees Algorithm ameliorates this by searching in many different areas simultaneously.}{fig:rtst}{0.66}

The Bees Algorithm has been applied to manufacturing cell formation, training neural networks for pattern recognition, scheduling jobs for a production machine, data clustering, and many others areas. See \cite{beesalg} for more examples and a comprehensive bibliography. However, to the best of our knowledge, the Bees Algorithm hasn't been adapted for the Vehicle Routing Problem until now.
